{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619731f2-81bc-4b9f-a283-e23f50c665ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "import aux_code\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390781e-98bd-44c2-93c8-586bd217bbe8",
   "metadata": {},
   "source": [
    "# Define folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87140945-97c8-42f1-8da8-69d85c438b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Folder containing the dataset\n",
    "dataset_folder = os.path.join('.','dataset')\n",
    "# Folder used to save the training history\n",
    "history_folder = os.path.join('.','history')\n",
    "aux_code.create_folder(history_folder)\n",
    "# Folder used to save the optimized MLP weights\n",
    "model_folder   = os.path.join('.','model')\n",
    "aux_code.create_folder(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db00e51-545d-4925-a231-801c2f87f346",
   "metadata": {},
   "source": [
    "# Define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29eac13e-8b58-4a25-ad0e-fb4b7861b23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 09:43:30.254425: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 09:43:30.254749: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = aux_code.define_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad58697-5e68-4a40-8dc6-b515e67bb899",
   "metadata": {},
   "source": [
    "# Load and normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d926e20-0c0c-4b61-bd00-c9b60034b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "training_set     = np.load(os.path.join(dataset_folder, 'training_set.npz'))\n",
    "x_train, y_train = aux_code.normalize_dataset(training_set['nn_data_train'], \n",
    "                                              training_set['stx_flare_loc_train'])\n",
    "\n",
    "# Validation set\n",
    "validation_set   = np.load(os.path.join(dataset_folder, 'validation_set.npz'))\n",
    "x_valid, y_valid = aux_code.normalize_dataset(validation_set['nn_data_valid'], \n",
    "                                              validation_set['stx_flare_loc_valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed42cea-29c6-4a1b-ad52-f51098cd1907",
   "metadata": {},
   "source": [
    "# Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e54b11-9ae5-4e6c-be05-44f1ce3b2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18145 samples, validate on 12824 samples\n",
      "Epoch 1/1000\n",
      "18000/18145 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00001: val_loss improved from inf to 0.00076, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 3s 142us/sample - loss: 0.0069 - val_loss: 7.5639e-04\n",
      "Epoch 2/1000\n",
      "17900/18145 [============================>.] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00002: val_loss improved from 0.00076 to 0.00057, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 90us/sample - loss: 0.0024 - val_loss: 5.6940e-04\n",
      "Epoch 3/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00003: val_loss improved from 0.00057 to 0.00048, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 89us/sample - loss: 0.0018 - val_loss: 4.8012e-04\n",
      "Epoch 4/1000\n",
      "18100/18145 [============================>.] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00004: val_loss improved from 0.00048 to 0.00045, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 88us/sample - loss: 0.0015 - val_loss: 4.5028e-04\n",
      "Epoch 5/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00005: val_loss did not improve from 0.00045\n",
      "18145/18145 [==============================] - 2s 92us/sample - loss: 0.0014 - val_loss: 4.9988e-04\n",
      "Epoch 6/1000\n",
      "18000/18145 [============================>.] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00006: val_loss did not improve from 0.00045\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 0.0012 - val_loss: 4.5165e-04\n",
      "Epoch 7/1000\n",
      "17600/18145 [============================>.] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00007: val_loss improved from 0.00045 to 0.00043, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 0.0012 - val_loss: 4.3422e-04\n",
      "Epoch 8/1000\n",
      "18100/18145 [============================>.] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00008: val_loss did not improve from 0.00043\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 0.0012 - val_loss: 4.5178e-04\n",
      "Epoch 9/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 0.0011\n",
      "Epoch 00009: val_loss did not improve from 0.00043\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 0.0011 - val_loss: 4.6509e-04\n",
      "Epoch 10/1000\n",
      "17700/18145 [============================>.] - ETA: 0s - loss: 0.0010\n",
      "Epoch 00010: val_loss improved from 0.00043 to 0.00042, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 90us/sample - loss: 0.0010 - val_loss: 4.2494e-04\n",
      "Epoch 11/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 0.0010\n",
      "Epoch 00011: val_loss did not improve from 0.00042\n",
      "18145/18145 [==============================] - 2s 102us/sample - loss: 0.0010 - val_loss: 4.2534e-04\n",
      "Epoch 12/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 9.7741e-04\n",
      "Epoch 00012: val_loss did not improve from 0.00042\n",
      "18145/18145 [==============================] - 2s 94us/sample - loss: 9.7799e-04 - val_loss: 4.5373e-04\n",
      "Epoch 13/1000\n",
      "17700/18145 [============================>.] - ETA: 0s - loss: 9.5430e-04\n",
      "Epoch 00013: val_loss improved from 0.00042 to 0.00040, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 9.5541e-04 - val_loss: 4.0163e-04\n",
      "Epoch 14/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 9.3760e-04\n",
      "Epoch 00014: val_loss improved from 0.00040 to 0.00039, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 9.3540e-04 - val_loss: 3.9407e-04\n",
      "Epoch 15/1000\n",
      "17900/18145 [============================>.] - ETA: 0s - loss: 8.7878e-04\n",
      "Epoch 00015: val_loss improved from 0.00039 to 0.00038, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 8.7837e-04 - val_loss: 3.7952e-04\n",
      "Epoch 16/1000\n",
      "17600/18145 [============================>.] - ETA: 0s - loss: 8.7073e-04\n",
      "Epoch 00016: val_loss did not improve from 0.00038\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 8.6822e-04 - val_loss: 3.8045e-04\n",
      "Epoch 17/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 8.6094e-04\n",
      "Epoch 00017: val_loss did not improve from 0.00038\n",
      "18145/18145 [==============================] - 2s 88us/sample - loss: 8.6507e-04 - val_loss: 3.9099e-04\n",
      "Epoch 18/1000\n",
      "18000/18145 [============================>.] - ETA: 0s - loss: 8.6091e-04\n",
      "Epoch 00018: val_loss improved from 0.00038 to 0.00035, saving model to ./model/model_weights.h5\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 8.5970e-04 - val_loss: 3.4590e-04\n",
      "Epoch 19/1000\n",
      "18100/18145 [============================>.] - ETA: 0s - loss: 8.3622e-04\n",
      "Epoch 00019: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 89us/sample - loss: 8.3649e-04 - val_loss: 3.9131e-04\n",
      "Epoch 20/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 8.2081e-04\n",
      "Epoch 00020: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 8.3036e-04 - val_loss: 4.0012e-04\n",
      "Epoch 21/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 8.3004e-04\n",
      "Epoch 00021: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 8.3025e-04 - val_loss: 4.3490e-04\n",
      "Epoch 22/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 8.2976e-04\n",
      "Epoch 00022: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 8.2436e-04 - val_loss: 3.5455e-04\n",
      "Epoch 23/1000\n",
      "18000/18145 [============================>.] - ETA: 0s - loss: 8.0792e-04\n",
      "Epoch 00023: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 92us/sample - loss: 8.0759e-04 - val_loss: 3.8064e-04\n",
      "Epoch 24/1000\n",
      "17700/18145 [============================>.] - ETA: 0s - loss: 8.1341e-04\n",
      "Epoch 00024: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 92us/sample - loss: 8.1229e-04 - val_loss: 4.0685e-04\n",
      "Epoch 25/1000\n",
      "17900/18145 [============================>.] - ETA: 0s - loss: 8.1041e-04\n",
      "Epoch 00025: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 89us/sample - loss: 8.0949e-04 - val_loss: 3.8380e-04\n",
      "Epoch 26/1000\n",
      "18100/18145 [============================>.] - ETA: 0s - loss: 7.9053e-04\n",
      "Epoch 00026: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 91us/sample - loss: 7.9013e-04 - val_loss: 3.7729e-04\n",
      "Epoch 27/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 7.8212e-04\n",
      "Epoch 00027: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 7.8184e-04 - val_loss: 3.8588e-04\n",
      "Epoch 28/1000\n",
      "17700/18145 [============================>.] - ETA: 0s - loss: 8.0823e-04\n",
      "Epoch 00028: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 8.0586e-04 - val_loss: 3.5311e-04\n",
      "Epoch 29/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 7.6632e-04\n",
      "Epoch 00029: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 89us/sample - loss: 7.6474e-04 - val_loss: 3.7008e-04\n",
      "Epoch 30/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 7.4250e-04\n",
      "Epoch 00030: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 84us/sample - loss: 7.5253e-04 - val_loss: 3.8381e-04\n",
      "Epoch 31/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 7.3414e-04\n",
      "Epoch 00031: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 7.4912e-04 - val_loss: 3.5545e-04\n",
      "Epoch 32/1000\n",
      "17600/18145 [============================>.] - ETA: 0s - loss: 7.6067e-04\n",
      "Epoch 00032: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 87us/sample - loss: 7.5693e-04 - val_loss: 3.6229e-04\n",
      "Epoch 33/1000\n",
      "17800/18145 [============================>.] - ETA: 0s - loss: 7.4108e-04\n",
      "Epoch 00033: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 90us/sample - loss: 7.3995e-04 - val_loss: 3.7442e-04\n",
      "Epoch 34/1000\n",
      "17500/18145 [===========================>..] - ETA: 0s - loss: 7.4219e-04\n",
      "Epoch 00034: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 7.4321e-04 - val_loss: 3.5341e-04\n",
      "Epoch 35/1000\n",
      "17600/18145 [============================>.] - ETA: 0s - loss: 7.5137e-04\n",
      "Epoch 00035: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 7.5157e-04 - val_loss: 3.7221e-04\n",
      "Epoch 36/1000\n",
      "17600/18145 [============================>.] - ETA: 0s - loss: 7.4846e-04\n",
      "Epoch 00036: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 86us/sample - loss: 7.4516e-04 - val_loss: 4.0768e-04\n",
      "Epoch 37/1000\n",
      "17400/18145 [===========================>..] - ETA: 0s - loss: 7.2306e-04\n",
      "Epoch 00037: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 85us/sample - loss: 7.1967e-04 - val_loss: 3.8116e-04\n",
      "Epoch 38/1000\n",
      "18000/18145 [============================>.] - ETA: 0s - loss: 7.3501e-04\n",
      "Epoch 00038: val_loss did not improve from 0.00035\n",
      "18145/18145 [==============================] - 2s 88us/sample - loss: 7.3568e-04 - val_loss: 3.8767e-04\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  \n",
    "                                                 factor=0.5,\n",
    "                                                 patience=10, \n",
    "                                                 min_lr=1e-6)  \n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_folder,'model_weights.h5'), \n",
    "                                                         save_best_only=True,\n",
    "                                                         monitor='val_loss',\n",
    "                                                         mode='min',\n",
    "                                                         verbose=1)\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                           patience=20,\n",
    "                                                           mode='min',\n",
    "                                                           verbose=1)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=1000, \n",
    "                    batch_size=100, \n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[reduce_lr,checkpoint_callback,early_stopping_callback])\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(history_folder,'training_history.pkl'), 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
